{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "748394da8a867656525da93a356544f0f15fb0021d7a843e967cb2c6876245f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed 고정\n",
    "random_seed=1\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.random.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data 종류\n(89355, 24)\nIndex(['신고번호', '신고일자', '통관지세관부호', '신고인부호', '수입자부호', '해외거래처부호', '특송업체부호',\n       '수입통관계획코드', '수입신고구분코드', '수입거래구분코드', '수입종류코드', '징수형태코드', '신고중량(KG)',\n       '과세가격원화금액', '운송수단유형코드', '반입보세구역부호', 'HS10단위부호', '적출국가코드', '원산지국가코드',\n       '관세율구분코드', '관세율', '검사결과코드', '우범여부', '핵심적발'],\n      dtype='object')\n제거 후 데이터 종류 Index(['통관지세관부호', '신고인부호', '특송업체부호', '수입통관계획코드', '수입신고구분코드', '수입거래구분코드',\n       '수입종류코드', '징수형태코드', '운송수단유형코드', '반입보세구역부호', '적출국가코드', '원산지국가코드',\n       '관세율구분코드'],\n      dtype='object')\n       통관지세관부호  신고인부호   특송업체부호 수입통관계획코드 수입신고구분코드  수입거래구분코드  수입종류코드  징수형태코드  \\\n89350       10  M9SYU   PR5UFJ        C        B        11      21      11   \n89351       41  T7VQN  missing        E        E        15      11      11   \n89352       40  7Q31W  missing        C        B        29      21      11   \n89353       40  UJ0JR   O04TIW        F        B        15      21      14   \n89354       30  4TUUB  missing        Z        B        15      21      11   \n\n       운송수단유형코드  반입보세구역부호 적출국가코드 원산지국가코드 관세율구분코드  \n89350        10   4002001     CN      CN      W2  \n89351        40   4077008     CN      CN    FCN1  \n89352        10   2086001     CN      CN       A  \n89353        40   1618003     CA      CA       C  \n89354        10   3078039     HK      HK       C  \n"
     ]
    }
   ],
   "source": [
    "train_origin_data=pd.read_csv('./data/custom_contest/train.csv')\n",
    "test_origin_data=pd.read_csv('./data/custom_contest/test.csv')\n",
    "'''\n",
    "0은 바꿀 필요가 있음 o는 숫자이므로 유지\n",
    "신고번호 = x\n",
    "신고일자 = x\n",
    "통관지세관부호 = o\n",
    "신고인부호 = 0\n",
    "수입자부호 = 0\n",
    "해외 거래처 부호 = 0\n",
    "특송업체부호 = 0 \n",
    "\n",
    "'''\n",
    "train_origin_data=train_origin_data.fillna('missing')\n",
    "# 데이터 확인\n",
    "print('Data 종류')\n",
    "print(train_origin_data.shape)\n",
    "print(train_origin_data.columns)\n",
    "# 쓸모없는 데이터 날리기\n",
    "train_origin_data.drop('신고번호',axis=1,inplace=True)\n",
    "train_origin_data.drop('신고일자',axis=1,inplace=True)\n",
    "train_origin_data.drop('수입자부호',axis=1,inplace=True)\n",
    "train_origin_data.drop('검사결과코드',axis=1,inplace=True)\n",
    "train_origin_data.drop('해외거래처부호',axis=1,inplace=True)\n",
    "train_origin_data.drop('HS10단위부호',axis=1,inplace=True)\n",
    "# target 두개 분리\n",
    "crime_target=torch.tensor(train_origin_data.pop('우범여부').to_numpy())#,dtype=torch.float)\n",
    "priority_target=torch.tensor(train_origin_data.pop('핵심적발').to_numpy())\n",
    "#numerical data 분리\n",
    "train_weight=np.log(train_origin_data.pop('신고중량(KG)').to_numpy()+1).reshape(-1,1)\n",
    "train_price=np.log(train_origin_data.pop('과세가격원화금액').to_numpy()+1).reshape(-1,1)\n",
    "train_custom_rate=train_origin_data.pop('관세율').to_numpy().reshape(-1,1)\n",
    "# 분리 확인\n",
    "print('제거 후 데이터 종류',train_origin_data.columns)\n",
    "print(train_origin_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "통관지세관부호 : (89355, 40)\n",
      "신고인부호 : (89355, 965)\n",
      "특송업체부호 : (89355, 81)\n",
      "수입통관계획코드 : (89355, 7)\n",
      "수입신고구분코드 : (89355, 4)\n",
      "수입거래구분코드 : (89355, 25)\n",
      "수입종류코드 : (89355, 10)\n",
      "징수형태코드 : (89355, 9)\n",
      "운송수단유형코드 : (89355, 6)\n",
      "반입보세구역부호 : (89355, 568)\n",
      "적출국가코드 : (89355, 89)\n",
      "원산지국가코드 : (89355, 94)\n",
      "관세율구분코드 : (89355, 35)\n"
     ]
    }
   ],
   "source": [
    "for key in train_origin_data.keys():\n",
    "    enc=OneHotEncoder().fit(train_origin_data[key].to_numpy().reshape(-1,1))\n",
    "    encoded_data=enc.transform(train_origin_data[key].to_numpy().reshape(-1,1))\n",
    "    print(key,':',encoded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoded dataset (89355, 1933)\ntorch.Size([89355, 1936])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# One hot encoding\n",
    "enc=OneHotEncoder(dtype=np.float32).fit(train_origin_data.to_numpy().reshape(-1,len(train_origin_data.columns)))\n",
    "train_encoded_data=enc.transform(train_origin_data.to_numpy().reshape(-1,len(train_origin_data.columns))).toarray()\n",
    "print(\"encoded dataset\",train_encoded_data.shape)\n",
    "\n",
    "# concat dataset\n",
    "train_price_tensor=torch.tensor(train_price,dtype=torch.float)\n",
    "train_weight_tensor=torch.tensor(train_weight,dtype=torch.float)\n",
    "train_custom_rate_tensor=torch.tensor(train_custom_rate,dtype=torch.float)\n",
    "train_encoded_data_tensor=torch.tensor(train_encoded_data,dtype=torch.float)\n",
    "train_tensor_data=torch.cat((train_encoded_data_tensor,train_price_tensor,train_weight_tensor,train_custom_rate_tensor),dim=1)\n",
    "# train_tensor_data=torch.cat((train_price_tensor,train_weight_tensor,train_custom_rate_tensor),dim=1)\n",
    "del train_price,train_weight,train_custom_rate,train_encoded_data\n",
    "print(train_tensor_data.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data 자르기\n",
    "batch_size=128\n",
    "test_split_rate=0.2\n",
    "indices=np.arange(len(train_tensor_data))\n",
    "crime_dataset=TensorDataset(train_tensor_data,crime_target)\n",
    "priority_dataset=TensorDataset(train_tensor_data,priority_target.float())\n",
    "# x,y train_index,test_index 보내기\n",
    "# model 구조\n",
    "train_indices,test_indices=train_test_split(indices,stratify=crime_target)\n",
    "np.save('./data/custom_contest/mod_data.npy',train_tensor_data.numpy())\n",
    "np.save('./data/custom_contest/mod_crime_target.npy',crime_target)\n",
    "np.save('./data/custom_contest/mod_priority_target.npy',priority_target)\n",
    "np.save('./data/custom_contest/mod_train_index.npy',train_indices)\n",
    "np.save('./data/custom_contest/mod_test_index.npy',test_indices)\n",
    "#crime\n",
    "crime_train_dataset=Subset(crime_dataset,train_indices)\n",
    "crime_test_dataset=Subset(crime_dataset,test_indices)\n",
    "crime_train_data_loader=DataLoader(crime_train_dataset,batch_size=batch_size,shuffle=True,)\n",
    "crime_test_data_loader=DataLoader(crime_train_dataset,batch_size=batch_size,shuffle=False)\n",
    "#priority\n",
    "priority_train_dataset=Subset(priority_dataset,train_indices)\n",
    "priority_test_dataset=Subset(priority_dataset,test_indices)\n",
    "priority_train_data_loader=DataLoader(priority_train_dataset,batch_size=batch_size,shuffle=True,)\n",
    "priority_test_data_loader=DataLoader(priority_train_dataset,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crime_model=nn.Sequential(nn.Linear(train_tensor_data.shape[1],5000),\n",
    "    nn.BatchNorm1d(5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000,1000),\n",
    "    nn.BatchNorm1d(1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000,100),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,2),\n",
    "    # nn.Sigmoid()\n",
    "    )\n",
    "# criterion=torch.nn.BCELoss()\n",
    "crime_criterion=torch.nn.CrossEntropyLoss()\n",
    "crime_optimizer=torch.optim.Adam(crime_model.parameters(),lr=1e-3,weight_decay=1e-4)\n",
    "epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1epoch 64256/67016, Accurcay: 79.30 Loss:0.44348\n",
      " Eval: 1 epoch, Accuracy: 79.01, Loss: 0.4317009210700297\n",
      "2epoch 6656/67016, Accurcay: 80.06 Loss:0.42172"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e2ca44b1bb7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m   \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m   \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-e2ca44b1bb7d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, train_data_loader, model)\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[0mcorrect\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m       \u001b[0mtrain_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m           \u001b[1;31m# print(outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch,train_data_loader,model,optimizer,criterion):\n",
    "  model.train()\n",
    "  total=0\n",
    "  correct=0.0\n",
    "  train_loss=0.0\n",
    "  for batch_idx, (data,targets) in enumerate(train_data_loader):\n",
    "      data,targets=data.cuda(),targets.cuda()\n",
    "      outputs=model(data)\n",
    "      loss=criterion(outputs,targets)#.view(-1,1))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # correct+=torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).sum()\n",
    "      correct+=torch.eq(torch.max(outputs,dim=1)[1],targets).sum()\n",
    "      total += targets.size(0)\n",
    "      train_loss+=loss.item()\n",
    "      if batch_idx%50==1:\n",
    "          # print(outputs)\n",
    "          # print(torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).view(-1))\n",
    "          print('\\r{}epoch {}/{}, Accurcay: {:.2f} Loss:{:.5f}'.format(epoch,total,len(train_data_loader.dataset),correct/total*100.0,train_loss/(batch_idx+1)),end='')\n",
    "\n",
    "def eval(epoch,test_data_loader,model,optimizer,criterion):\n",
    "    model.eval()\n",
    "    eval_loss=0.0\n",
    "    correct=0.0\n",
    "    total=0.0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx,(data,targets) in enumerate(test_data_loader):\n",
    "          data,targets=data.cuda(),targets.cuda()\n",
    "          outputs=model(data)\n",
    "          loss=criterion(outputs,targets)#.view(-1,1))\n",
    "          # correct+=torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).sum()\n",
    "          correct+=torch.eq(torch.max(outputs,dim=1)[1],targets).sum()\n",
    "          total += targets.size(0)\n",
    "          eval_loss +=loss.item()\n",
    "\n",
    "    print('\\n Eval: {} epoch, Accuracy: {:.2f}, Loss: {}'.format(epoch,correct/total*100.0,eval_loss/(batch_idx+1)))\n",
    "\n",
    "\n",
    "crime_model.cuda()\n",
    "for epoch in range(1,epochs+1):\n",
    "  train(epoch,crime_train_data_loader,crime_model,crime_optimizer,crime_criterion)\n",
    "  eval(epoch,crime_test_data_loader,crime_model,crime_optimizer,crime_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_model=nn.Sequential(nn.Linear(train_tensor_data.shape[1],5000),\n",
    "    nn.BatchNorm1d(5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000,1000),\n",
    "    nn.BatchNorm1d(1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000,100),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,1)\n",
    ")\n",
    "priority_criterion=torch.nn.MSELoss()\n",
    "priority_optimizer=torch.optim.Adam(priority_model.parameters(),lr=1e-3,weight_decay=1e-4)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1epoch 64256/67016, Accurcay: 72.77 Loss:0.31335\n",
      " Eval: 1 epoch, Accuracy: 68.90, Loss: 0.3111581713074946\n",
      "2epoch 64256/67016, Accurcay: 74.15 Loss:0.29433\n",
      " Eval: 2 epoch, Accuracy: 77.92, Loss: 0.3166913243138608\n",
      "3epoch 64256/67016, Accurcay: 76.24 Loss:0.27248\n",
      " Eval: 3 epoch, Accuracy: 77.70, Loss: 0.2554301707969822\n",
      "4epoch 64256/67016, Accurcay: 77.58 Loss:0.25226\n",
      " Eval: 4 epoch, Accuracy: 76.60, Loss: 0.27607957105941444\n",
      "5epoch 64256/67016, Accurcay: 78.83 Loss:0.23214\n",
      " Eval: 5 epoch, Accuracy: 81.93, Loss: 0.26851120308212195\n",
      "6epoch 64256/67016, Accurcay: 80.16 Loss:0.21633\n",
      " Eval: 6 epoch, Accuracy: 83.75, Loss: 0.19827706671056858\n",
      "7epoch 64256/67016, Accurcay: 81.23 Loss:0.20167\n",
      " Eval: 7 epoch, Accuracy: 84.52, Loss: 0.2201266085856971\n",
      "8epoch 64256/67016, Accurcay: 81.80 Loss:0.19106\n",
      " Eval: 8 epoch, Accuracy: 85.43, Loss: 0.17858036082835144\n",
      "9epoch 64256/67016, Accurcay: 82.26 Loss:0.18235\n",
      " Eval: 9 epoch, Accuracy: 83.76, Loss: 0.22383690190337996\n",
      "10epoch 64256/67016, Accurcay: 83.04 Loss:0.16990\n",
      " Eval: 10 epoch, Accuracy: 86.55, Loss: 0.15811749780906065\n",
      "11epoch 64256/67016, Accurcay: 82.75 Loss:0.16687\n",
      " Eval: 11 epoch, Accuracy: 82.76, Loss: 0.19871417694646892\n",
      "12epoch 64256/67016, Accurcay: 82.94 Loss:0.16171\n",
      " Eval: 12 epoch, Accuracy: 86.66, Loss: 0.16027398171429416\n",
      "13epoch 64256/67016, Accurcay: 82.99 Loss:0.15771\n",
      " Eval: 13 epoch, Accuracy: 86.45, Loss: 0.18152161234037112\n",
      "14epoch 64256/67016, Accurcay: 83.20 Loss:0.15111\n",
      " Eval: 14 epoch, Accuracy: 86.55, Loss: 0.20708964037076208\n",
      "15epoch 64256/67016, Accurcay: 82.85 Loss:0.14663\n",
      " Eval: 15 epoch, Accuracy: 84.29, Loss: 0.18809504458638093\n",
      "16epoch 64256/67016, Accurcay: 83.00 Loss:0.14320\n",
      " Eval: 16 epoch, Accuracy: 85.09, Loss: 0.2861512959543758\n",
      "17epoch 64256/67016, Accurcay: 82.90 Loss:0.13950\n",
      " Eval: 17 epoch, Accuracy: 85.88, Loss: 0.16504178042630202\n",
      "18epoch 64256/67016, Accurcay: 82.75 Loss:0.13836\n",
      " Eval: 18 epoch, Accuracy: 86.15, Loss: 0.20180128571641354\n",
      "19epoch 64256/67016, Accurcay: 82.50 Loss:0.13677\n",
      " Eval: 19 epoch, Accuracy: 87.64, Loss: 0.3403087993870482\n",
      "20epoch 64256/67016, Accurcay: 82.20 Loss:0.13236\n",
      " Eval: 20 epoch, Accuracy: 85.07, Loss: 0.26586394242316713\n",
      "21epoch 64256/67016, Accurcay: 82.24 Loss:0.13024\n",
      " Eval: 21 epoch, Accuracy: 85.94, Loss: 0.1677220523755514\n",
      "22epoch 64256/67016, Accurcay: 82.55 Loss:0.12929\n",
      " Eval: 22 epoch, Accuracy: 85.61, Loss: 0.218847585856232\n",
      "23epoch 64256/67016, Accurcay: 82.69 Loss:0.12582\n",
      " Eval: 23 epoch, Accuracy: 85.44, Loss: 0.41659334048856306\n",
      "24epoch 64256/67016, Accurcay: 82.12 Loss:0.12566\n",
      " Eval: 24 epoch, Accuracy: 84.86, Loss: 0.18600677742928495\n",
      "25epoch 64256/67016, Accurcay: 81.05 Loss:0.12330\n",
      " Eval: 25 epoch, Accuracy: 81.47, Loss: 0.21257818410642274\n",
      "26epoch 64256/67016, Accurcay: 80.29 Loss:0.12160\n",
      " Eval: 26 epoch, Accuracy: 84.01, Loss: 0.18071465617941537\n",
      "27epoch 64256/67016, Accurcay: 79.99 Loss:0.11975\n",
      " Eval: 27 epoch, Accuracy: 84.55, Loss: 0.20506041536804373\n",
      "28epoch 64256/67016, Accurcay: 78.74 Loss:0.11905\n",
      " Eval: 28 epoch, Accuracy: 82.10, Loss: 0.22160507127187634\n",
      "29epoch 64256/67016, Accurcay: 79.11 Loss:0.11700"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-47cccf91c681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mpriority_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m   \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_train_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_criterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m   \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_test_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpriority_criterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-47cccf91c681>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, train_data_loader, model, optimizer, criterion)\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[0mcorrect\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m       \u001b[0mtrain_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m           \u001b[1;31m# print(outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch,train_data_loader,model,optimizer,criterion):\n",
    "  model.train()\n",
    "  total=0\n",
    "  correct=0.0\n",
    "  train_loss=0.0\n",
    "  for batch_idx, (data,targets) in enumerate(train_data_loader):\n",
    "      data,targets=data.cuda(),targets.cuda()\n",
    "      outputs=model(data)\n",
    "      loss=criterion(torch.clip(outputs,0,2),targets.view(-1,1))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # correct+=torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).sum()\n",
    "      correct+=torch.eq(torch.round(outputs),targets.view(-1,1)).sum()\n",
    "      total += targets.size(0)\n",
    "      train_loss+=loss.item()\n",
    "      if batch_idx%50==1:\n",
    "          # print(outputs)\n",
    "          # print(torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).view(-1))\n",
    "          print('\\r{}epoch {}/{}, Accurcay: {:.2f} Loss:{:.5f}'.format(epoch,total,len(train_data_loader.dataset),correct/total*100.0,train_loss/(batch_idx+1)),end='')\n",
    "\n",
    "def eval(epoch,test_data_loader,model,optimizer,criterion):\n",
    "    model.eval()\n",
    "    eval_loss=0.0\n",
    "    correct=0.0\n",
    "    total=0.0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx,(data,targets) in enumerate(test_data_loader):\n",
    "          data,targets=data.cuda(),targets.cuda()\n",
    "          outputs=model(data)\n",
    "          loss=criterion(outputs,targets.view(-1,1))\n",
    "          # correct+=torch.eq(torch.ge(outputs,0.5).float(),targets.view(-1,1)).sum()\n",
    "          correct+=torch.eq(torch.round(outputs),targets.view(-1,1)).sum()\n",
    "          total += targets.size(0)\n",
    "          eval_loss +=loss.item()\n",
    "\n",
    "    print('\\n Eval: {} epoch, Accuracy: {:.2f}, Loss: {}'.format(epoch,correct/total*100.0,eval_loss/(batch_idx+1)))\n",
    "\n",
    "\n",
    "priority_model.cuda()\n",
    "for epoch in range(1,epochs+1):\n",
    "  train(epoch,priority_train_data_loader,priority_model,priority_optimizer,priority_criterion)\n",
    "  eval(epoch,priority_test_data_loader,priority_model,priority_optimizer,priority_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}